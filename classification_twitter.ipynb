{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting/Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport csv\n\nfrom textblob import TextBlob\nimport re\n\nimport nltk\n# nltk.download()\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger') \nnltk.download('vader_lexicon')\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk import pos_tag, pos_tag_sents\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.read_csv(\"../input/stockticker10tweets5321-label/stockticker10-tweets(5-3-21) label.csv\")\ndf = pd.read_csv(\"../input/stockticker10tweets/stockticker10-tweets.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Datetime(UTC)','Tweet ID','RT', 'RT Count', 'Fav Count'], axis=1)\ndf.dropna(subset=['Sentiment (2pos/1neu/0neg)'], inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\ndef preprocess(text):\n    text = str(text)\n    text = text.lower()\n    text = text.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', text)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    rem_tweet = re.sub('@[^\\s]+','',rem_num)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_tweet)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stop]\n    stem_words=[stemmer.stem(w) for w in filtered_words]\n    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n    return \" \".join(filtered_words)\n\ndf['cleanText'] = df['Text'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleanText = df['cleanText']\ndf_cleanText","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification\n\n## VADER"},{"metadata":{"trusted":true},"cell_type":"code","source":"def VADER(sentence):\n    VADER_sentence = str(sentence)\n    VADER_analyser = SentimentIntensityAnalyzer()\n    \n    \n    score = VADER_analyser.polarity_scores(VADER_sentence)\n    VADER_answer = score['compound']\n    if(VADER_answer>0):\n        return_rating = 2\n    elif(VADER_answer<0):\n        return_rating = 0\n    else:\n        return_rating = 1\n    return return_rating\n    \n\ndf['VADER'] = df['Text'].map(lambda s:VADER(s)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing VADER against manual label"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=pd.Series(df['Sentiment (2pos/1neu/0neg)']).array\ny_pred = pd.Series(df['VADER']).array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nsns.heatmap(cm, annot = True, fmt='d')\nplt.title(\"Confusion matrix of VADER vs Manual Label for Twitter\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y_true, y_pred)\nf1Score = f1_score(y_true, y_pred, average='macro')\nprecisionScore = precision_score(y_true, y_pred, average='macro')\nrecallScore = recall_score(y_true, y_pred, average='macro')\n\nprint(\"Accuracy: {}%\\nF1-score: {}\\nPrecision Score: {}\\nRecall Score: {}\".format(f'{accuracy*100:.2f}', f'{f1Score:.3f}', f'{precisionScore:.3f}', f'{recallScore:.3f}'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df[['cleanText']], \n                                                    df['Sentiment (2pos/1neu/0neg)'], \n                                                    test_size=0.2, \n                                                    random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(ngram_range=(1,2))\ncv = vectorizer.fit_transform(x_train['cleanText'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB().fit(cv, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vector = vectorizer.transform(x_test['cleanText'])\nresult = clf.predict(test_vector)\ncmNB = metrics.confusion_matrix(y_test, result)\ncmNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nsns.heatmap(cmNB, annot = True, fmt='d')\nplt.title(\"Confusion matrix of Naive Bayes Classification for Twitter\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count vectorizing with unigram + bigram\naccuracy = accuracy_score(y_test, result)\nf1Score = f1_score(y_test, result, average='macro')\nprecisionScore= precision_score(y_test, result, average='macro')\nrecallScore = recall_score(y_test, result, average='macro')\n\nprint(\"Accuracy: {}%\\nF1-score: {}\\nPrecision Score: {}\\nRecall Score: {}\".format(f'{accuracy*100:.2f}', f'{f1Score:.3f}', f'{precisionScore:.3f}', f'{recallScore:.3f}'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Classification\n## Random Forest Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(bootstrap=False,max_features='sqrt',n_estimators=800)\n\nrfCLF = clf.fit(cv, y_train)\n\ntest_vector = vectorizer.transform(x_test['cleanText'])\nresult = clf.predict(test_vector)\ncmRF = metrics.confusion_matrix(y_test, result)\ncmRF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nsns.heatmap(cmNB, annot = True, fmt='d')\nplt.title(\"Confusion matrix of Random Forest Classification for Twitter\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count vectorizing with unigram + bigram\naccuracy = accuracy_score(y_test, result)\nf1Score = f1_score(y_test, result, average='macro')\nprecisionScore= precision_score(y_test, result, average='macro')\nrecallScore = recall_score(y_test, result, average='macro')\n\nprint(\"Accuracy: {}%\\nF1-score: {}\\nPrecision Score: {}\\nRecall Score: {}\".format(f'{accuracy*100:.2f}', f'{f1Score:.3f}', f'{precisionScore:.3f}', f'{recallScore:.3f}'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('twitter_dataset.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}